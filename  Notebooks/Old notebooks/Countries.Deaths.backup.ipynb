{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country project (death data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"/Users/gloftus/covid-king/\")\n",
    "\n",
    "isos = ['ISR', 'JOR', 'LBN', 'IRQ', 'IRN', 'SYR', 'ARE', 'OMN', 'YEM']\n",
    "rows_cols = (3, 4)\n",
    "filename = \"figs/country_deaths.png\"\n",
    "Geofffile = \"data/CentralAmerica.deaths.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import io\n",
    "import ast\n",
    "import warnings\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "import pandas as pd; pd.set_option('display.max_rows', 500); pd.set_option('display.float_format', '{:.10f}'.format)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def iso_to_country(iso):\n",
    "    \"\"\"\n",
    "    Eats an ISO, pops out a country.\n",
    "    \"\"\"\n",
    "    return iso_mapping[iso]\n",
    "\n",
    "def country_to_iso(country):\n",
    "    \"\"\"\n",
    "    Eats a country, pops out an ISO.\n",
    "    \"\"\"\n",
    "    return iso_mapping_r[country]\n",
    "\n",
    "def get_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    data_raw = response.content.decode('utf-8')\n",
    "    data_raw_str = io.StringIO(data_raw)\n",
    "    df_countries = pd.read_csv(data_raw_str, index_col=\"date\", parse_dates=True)\n",
    "    return df_countries\n",
    "\n",
    "def get_data_country():\n",
    "    # initial parameters\n",
    "    url = \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"\n",
    "    countries_filename = Path(\"countries_data.csv\")\n",
    "    yesterday = pd.Timestamp(datetime.date.today() - datetime.timedelta(days=1))\n",
    "    kwargs = dict(index_col=\"date\", parse_dates=True)\n",
    "    \n",
    "    # read from file\n",
    "    if not countries_filename.is_file():\n",
    "        df = get_from_url(url)\n",
    "        df.to_csv(countries_filename)\n",
    "        return df\n",
    "    else:\n",
    "        df = pd.read_csv(countries_filename, **kwargs)\n",
    "\n",
    "    # check if dataframe is completely up-to-date.\n",
    "    # if not, refresh it.\n",
    "    last_date = pd.Timestamp(df.iloc[-1].name)\n",
    "    updated = last_date >= yesterday\n",
    "    if not updated:\n",
    "        df = get_from_url(url)\n",
    "        df.to_csv(countries_filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_data_country()\n",
    "iso_mapping = df[[\"iso_code\", \"location\"]].drop_duplicates().set_index(\"iso_code\").squeeze()\n",
    "iso_mapping.index.name = None\n",
    "iso_mapping_r = pd.Series(iso_mapping.index.values, index=iso_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE THE PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "---\n",
    "- pass in a list of ISOs at the top\n",
    "- if desired, pass in the number of rows and columns you want in the figure with `rows_cols = (rownumber, columnnumber)`\n",
    "- if `rows_cols = (None, None)`, then the script will default to 4 columns and a dynamic number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8655203 10203567  6825309 40221160 83987424 17505522  9890611  5106575\n",
      " 29825390]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a83dc2b19d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Actually run the program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a83dc2b19d03>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(isos, rows_cols, filename)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# add regression prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mregression_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_day_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# --- Plotting --- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a83dc2b19d03>\u001b[0m in \u001b[0;36mpredict_future\u001b[0;34m(df, future_day_count)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlast_7_days\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_7_days\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_day_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuture_day_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a83dc2b19d03>\u001b[0m in \u001b[0;36mlinear_regression\u001b[0;34m(country_col, future_day_count)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Fit out linear regression model on the previous 7 days, then predict the next `future_day_count` days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Give back the dates that we predicted, and the predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,\n\u001b[0;32m--> 519\u001b[0;31m                                    y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0;32m--> 881\u001b[0;31m                         ensure_2d=False, dtype=None)\n\u001b[0m\u001b[1;32m    882\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 721\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n\u001b[0;32m--> 106\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "def get_population(iso_code, data):\n",
    "    \"\"\"\n",
    "    Get the population for a particular country, based on total deaths and deaths per million.\n",
    "    \"\"\"\n",
    "    df = data[data.iso_code == iso_code]  # grab only the rows corresponding to the specific iso code\n",
    "    pops = df[\"total_deaths\"] / df[\"total_deaths_per_million\"]\n",
    "    return int(pops.mean() * 1000000)\n",
    "    \n",
    "def get_matrix1(ordered_iso_codes, data, iso_mapping, filename=\"\"):\n",
    "    \"\"\"\n",
    "    ``data`` is your entire dataframe (the whole thing)\n",
    "    \n",
    "    ``ordered_iso_codes`` looks like, e.g., ['IRL','GBR', 'PRT'].\n",
    "        You put the iso codes in the order you want and this function outputs a DataFrame\n",
    "        which has the full country names in the same order.\n",
    "        \n",
    "    ``save_csv`` is a filename if you want to save it, otherwise just don't use it.\n",
    "    \"\"\"\n",
    "    region = data.query(f\"iso_code in {ordered_iso_codes}\")\n",
    "    region = region.pivot(columns='iso_code', values='total_deaths')\n",
    "    region = region[ordered_iso_codes]\n",
    "    region.rename(iso_mapping[region.columns].to_dict(), axis=1, inplace=True)\n",
    "    region = region[region.index > \"2020-03-25\"]\n",
    "    region.columns.name = None\n",
    "    if filename:\n",
    "        region.to_csv(filename)\n",
    "    region.to_csv(Geofffile) # Save the matrix for Geoff\n",
    "    return region\n",
    "\n",
    "def get_matrix2(matrix1, iso_list, data):\n",
    "    populations = np.array([get_population(country, data=data) for country in iso_list])\n",
    "    print(populations)\n",
    "    return matrix1.diff().rolling(14).mean() / (populations/1000000)\n",
    "\n",
    "def get_matrix3(matrix2):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return np.log10(matrix2)\n",
    "\n",
    "def linear_regression(country_col, future_day_count):\n",
    "    # pred_idx is a list of the next `future_day_count` days, starting tomorrow.\n",
    "    # Each of the elements are dates, not numbers.\n",
    "    pred_idx = pd.date_range(start=country_col.index[-1] + pd.Timedelta(days=1), \n",
    "                             periods=future_day_count, freq=\"D\")\n",
    "    \n",
    "    # days_from_start is a list of numbers, each of which denotes one of the previous 7 days,\n",
    "    # starting with 0 (0 representing 7 days ago)\n",
    "    days_from_start = (country_col.index - country_col.index[0]).days\n",
    "    \n",
    "    # the integer-day index for tomorrow.\n",
    "    tomorrow = days_from_start[-1] + 1\n",
    "    \n",
    "    # the integer-day indices for the days we want to predict.\n",
    "    to_pred = np.arange(tomorrow, tomorrow + future_day_count).reshape(-1, 1)\n",
    "    \n",
    "    # Get the pure integer indices for the days that we're fitting the linear regression on,\n",
    "    # in integers, and then format properly\n",
    "    X = days_from_start.values.reshape(-1, 1)\n",
    "    \n",
    "    # get the actual values for the days we're fitting the linear regression on, then format properly\n",
    "    Y = country_col.values.reshape(-1, 1)\n",
    "    \n",
    "    # Fit out linear regression model on the previous 7 days, then predict the next `future_day_count` days\n",
    "    y_pred = LinearRegression().fit(X, Y).predict(to_pred)\n",
    "    \n",
    "    # Give back the dates that we predicted, and the predicted values\n",
    "    return pred_idx, y_pred.squeeze()\n",
    "\n",
    "def predict_future(df, future_day_count=14):\n",
    "    last_7_days = df[-7:]\n",
    "    df_dict = {}\n",
    "    for col in last_7_days:\n",
    "        series = last_7_days[col]\n",
    "        idx, pred = linear_regression(series, future_day_count=future_day_count)\n",
    "        df_dict[col] = pred\n",
    "\n",
    "    return 10**pd.DataFrame(df_dict, index=idx)\n",
    "\n",
    "def main(isos, rows_cols=(None, None), filename=\"\"):\n",
    "    \n",
    "    # grab matrices\n",
    "    matrix1 = get_matrix1(isos, data=df, iso_mapping=iso_mapping)\n",
    "    matrix2 = get_matrix2(matrix1, isos, data=df)\n",
    "    matrix3 = get_matrix3(matrix2)\n",
    "    final = 10**matrix3\n",
    "    \n",
    "    # add regression prediction\n",
    "    regression_predictions = predict_future(matrix3, future_day_count=14)\n",
    "    \n",
    "    # --- Plotting --- #\n",
    "    \n",
    "    # By default, create the grid dynamically with 4 columns and changing \n",
    "    # number of rows. If you want, you can pass a specific number of rows\n",
    "    # and columns by passing (rows_cols) to this function.\n",
    "    n_plots = len(isos)\n",
    "    if rows_cols == (None, None):\n",
    "        ncols = 4\n",
    "        quotient, remainder = divmod(n_plots, ncols)\n",
    "        nrows = quotient if remainder==0 else quotient+1\n",
    "    else:\n",
    "        nrows, ncols = rows_cols\n",
    "        quotient, remainder = divmod(n_plots, ncols)\n",
    "        \n",
    "    # set up figure by making an empty grid of plots, each of which will get \n",
    "    # data added to it in the country loop\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*4, nrows*3.5))\n",
    "        \n",
    "    # get rid of any plots we aren't using\n",
    "    if remainder != 0:\n",
    "        diff = len(axs.flat) - len(isos)\n",
    "        for ax in axs.flat[-diff:]:\n",
    "            ax.set_axis_off()\n",
    "    \n",
    "    # loop through the countries we're plotting, and make a \n",
    "    # plot for each country\n",
    "    for i, iso in enumerate(isos):\n",
    "        \n",
    "        # get the name of the country and create a scatterplot\n",
    "        country_name = iso_mapping[iso]\n",
    "        ax = axs.flat[i]\n",
    "        g = sns.scatterplot(data=final, x=final.index, y=country_name, \n",
    "                            ax=ax, color=\"black\", edgecolor=\"none\", s=8, )\n",
    "        \n",
    "        # add regression line to the current country plot and make blue\n",
    "        sns.lineplot(data=regression_predictions, x=regression_predictions.index, \n",
    "                     y=country_name, ax=g, color=\"blue\")\n",
    "        \n",
    "        \n",
    "        # add title and set y-axis to logarithmic scale\n",
    "        g.set_title(f\"{country_name}\", fontsize=18)\n",
    "        g.set(yscale=\"log\", ylabel=\"NXPC\", ylim=(.1, 200))\n",
    "        \n",
    "        # mess with x-axis ticks\n",
    "        g.xaxis.set_minor_locator(mdates.WeekdayLocator(interval=2))  # change the interval for the smaller ticks on x-axis to every 2 days\n",
    "        g.xaxis.set_major_locator(mdates.WeekdayLocator(interval=8))  # change the interval for the larger ticks on x-axis to every 8 days\n",
    "        g.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))  # show x-ticks in month-day format, don't display years\n",
    "        g.xaxis.label.set_size(14)  # sets size of x-axis label\n",
    "        g.tick_params(axis='x', labelrotation=45)  # rotate ticks by 45 degrees\n",
    "        \n",
    "        # mess with y-axis ticks\n",
    "        g.yaxis.set_major_formatter(ScalarFormatter())  # ticks in integers rather than scientific notation\n",
    "        g.yaxis.label.set_size(14)  # sets size of y-axis label\n",
    "        \n",
    "        # add green horizontal line\n",
    "        g.axhline(y=.7, color=\"green\")\n",
    "        \n",
    "        # add horizontal grid lines, make the color black, give them transparency of 0.1\n",
    "        g.yaxis.grid(which='major', color='black', alpha=0.2)\n",
    "        \n",
    "    # Increase the vertical (height) distance between plots on the grid\n",
    "    plt.tight_layout(h_pad=2.5)\n",
    "    \n",
    "    # save the figure\n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\", dpi=144)\n",
    "    \n",
    "# Actually run the program\n",
    "main(isos, rows_cols=rows_cols, filename=filename)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dad Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Region Massachusetts New York Maryland North Carolina Florida   Penn  \\\n",
      "0  Population          6.94    19.45     6.05          10.49   21.48   12.8   \n",
      "1      26-Mar         2,417   39,058      581            639   2,478  1,690   \n",
      "2      27-Mar         3,240   44,746      790            784   3,198  2,217   \n",
      "3      28-Mar         4,257   53,517    1,066            972   4,038  2,815   \n",
      "4      29-Mar         4,955   59,783    1,244          1,166   4,943  3,441   \n",
      "\n",
      "  Indiana Illinois Nevada Washington Oregon Nothern California  \\\n",
      "0    6.73    12.67   3.08       7.61   4.22              15.73   \n",
      "1     661    2,538    535      3,208    316              1,888   \n",
      "2     992    3,029    621      3,770    415              2,212   \n",
      "3   1,239    3,547    738      4,311    480              2,403   \n",
      "4   1,522    4,615    920      4,897    549              2,605   \n",
      "\n",
      "  Southern California Canada   Italy United Kingdom  \n",
      "0               23.64  38.01   60.32          67.89  \n",
      "1               2,172  4,042  80,589         20,970  \n",
      "2               2,703  4,682  86,498         24,219  \n",
      "3               3,163  5,576  92,472         27,062  \n",
      "4               3,716  6,280  97,689         29,954  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Example.Matrix1.csv')\n",
    "print(df.head())\n",
    "\n",
    "# TestMatrix1 = pandas.read_csv('Example.Matrix1.csv')\n",
    "# TestMatrix1.shape\n",
    "# print(TestMatrix1)\n",
    "\n",
    "\n",
    "# #import necessary modules\n",
    "# import csv\n",
    "\n",
    "# reader = csv.DictReader(open(\"Example.Matrix1.csv\"))\n",
    "# for raw in reader:\n",
    "#     print(raw)\n",
    "\n",
    "# #import necessary modules\n",
    "# import csv\n",
    "# with open('Example.Matrix1.csv','rt')as f:\n",
    "#   data = csv.reader(f)\n",
    "#   for row in data:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: finding ISO codes\n",
    "\n",
    "Use the two cells below to find ISOs from country names or vice-versa.\n",
    "- the `iso_mapping` variable contains the entire list of mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COL', 'VEN', 'BOL', 'ECU', 'PER', 'BRA', 'ARG', 'PRY', 'URY', 'CHL']\n"
     ]
    }
   ],
   "source": [
    "# You can get isos from countries with\n",
    "# isos = list(country_to_iso([\"Ireland\", \"United Kingdom\", \"Portugal\", \"Spain\", \"France\", 'Italy', 'Belgium', 'Luxembourg']))\n",
    "# isos = list(country_to_iso([\"Ireland\", \"United Kingdom\", \"Portugal\", \"Spain\", \"France\", \"Italy\", \"Belgium\", \"Luxembourg\", \"Denmark\"]))\n",
    "\n",
    "# Or just pass in a single country:\n",
    "# isos = isos_from_countries = list(country_to_iso([\"Brazil\", \"Argentina\",\"Chile\", \"Peru\", \"Bolivia\", \"Paraguay\", \"Uruguay\", \"Venezuela\", \"Colombia\"]))\n",
    "# isos = isos_from_countries = list(country_to_iso([\"Mexico\", \"Belize\",\"Guatemala\", \"El Salvador\", \"Honduras\", \"Nicaragua\", \"Costa Rica\", \"Panama\"]))\n",
    "isos = isos_from_countries = list(country_to_iso([\"Colombia\", \"Venezuela\",\"Bolivia\", \"Ecuador\", \"Peru\", \"Brazil\", \"Argentina\", \"Paraguay\", \"Uruguay\", \"Chile\"]))\n",
    "\n",
    "print(isos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Colombia', 'Venezuela', 'Bolivia', 'Ecuador', 'Peru', 'Brazil', 'Argentina', 'Paraguay', 'Uruguay', 'Chile']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mexico'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get countries from isos with\n",
    "country_names = list(iso_to_country(isos))\n",
    "print(country_names)\n",
    "\n",
    "# Or a single iso:\n",
    "iso_to_country(\"MEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from when I made the functions all separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for daily regions (cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will grab the current data and cache it into a local .csv file.  \n",
    "\n",
    "If you already have the local csv file and it's up-to-date (meaning, the most recent date in it is greater than or equal to yesterday), just use the local csv file to avoid re-downloading uneccesarily.\n",
    "\n",
    "If you don't already have the local csv file, download it as well.\n",
    "\n",
    "**useful things**\n",
    "- check out the variable `iso_mapping`. It maps iso codes to country names.\n",
    "- You can use this to look at particular country codes. For example, you could type `iso_mapping[['HUN', 'EST']]` to look at the iso conversions for Hungary and Estonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates Matrix 1 for each cell. Feel free to peruse it. You just pass a list of ISO codes into `convert`. More information in the string underneath the `convert` definition (`def`).\n",
    "\n",
    "**useful things**\n",
    "- the `iso_to_country` function eats an ISO and outputs a country name. For example, iso_to_country(\"IRL\") returns \"Ireland\".\n",
    "- the `country_to_iso` function eats a country and outputs an ISO. For example, country_to_iso(\"Ireland\") returns \"IRL\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates matrix 2. There are small disparity between these numbers and the ones in the Excel spreadsheet -- that's because I calculated population manually based on the original countries_data, rather than using the numbers in the sheet. \n",
    "\n",
    "If you'd like you can replace `populations = np.array([get_population(country) for country in iso_list])` with `populations = np.array([4.977, 67.886, 10.296, 47.431, 67.081, 60.317, 11.493, 0.626])` for e.g. EU1. \n",
    "\n",
    "I personally think this is too much of a hassle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fun part!\n",
    "\n",
    "Thoughts:\n",
    "- Lasso Regression or Elastic Regression is almost always better to use than normal linear regression. I can swap those out very easily if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "us_state_abbrev_r = {v: k for k, v in us_state_abbrev.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
